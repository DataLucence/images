{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing strategies: vertical and horizontal integration\n",
    "\n",
    "Congratulations, you've just completed an awesome image processing pipeline that takes an image and does something rather useful. You've tested it a few times and you're happy with the results. Now you need to apply the same operation to 1000 images. Maybe you need to apply them to 20 datasets with slightly different parameters. Maybe you need to aggreagate information across the results of each processed image to make your conclusions. Doing this can be hard, but sometimes it's so easy you can see many ways to approach this, and are not sure which one is the best. Let's talk about what you need to keep in mind when building systems to actually apply your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** You're at the point where you have the functions below. How might you apply the pipeline you developed to every image in a folder, saving the results in a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "\n",
    "def loading(image_file_name):\n",
    "    return imread(image_file_name)\n",
    "\n",
    "def preprocessing(image):\n",
    "    return image # real code would go here\n",
    "\n",
    "def info_extraction(image):\n",
    "    results = None\n",
    "    return results\n",
    "\n",
    "data_folder = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(data_folder)\n",
    "\n",
    "files = [f for f in listdir(data_folder) if '.tif' in f]\n",
    "results = []\n",
    "\n",
    "for f in files:\n",
    "    img = loading(f)\n",
    "    processed = preprocessing(img)\n",
    "    result = info_extraction(processed)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What made this a pretty straightforward proceedure is that we encapsulated _everything_ in a function. If we developed our pipeline without doing this, we could wrap everthing in a giant function which took as parameters everything we needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whole_pipeline(image_file_name):\n",
    "    pass # code would go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not be the best idea. Maybe next week you want to load similar images the same way, preprocess them differently, then extract the same information. You could have imported the `loading` and `info_extraction` you already have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also be thinking of an alternative way of solving this problem that looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = [f for f in listdir(data_folder) if '.tif' in f]\n",
    "images = []\n",
    "processed = []\n",
    "results = []\n",
    "\n",
    "for f in files:\n",
    "    images.append(loading(f))\n",
    "    \n",
    "for img in images:\n",
    "    processed.append(preprocessing(img))\n",
    "    \n",
    "for proc in processed:\n",
    "    results.append(info_extraction(proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of applying all processing steps to each image one at a time, you can apply each processing step to each image. So now rather than completing your pipline \"vertically\", from start to finish, top to bottom, you are completing your pipeline \"horizontally\", applying each stage across the board in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: graphics of horizontal and vertical strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, when you've made your code modular with little functions, Python tries to support this approach with _functional programming tools_. We'll talk about one of the especially useful ones:\n",
    "\n",
    "- `map(function, iterable)`. Applies `function` to each element in `iterable` (i.e. anything you can loop through) and gives you a new list with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = [f for f in listdir(data_folder) if '.tif' in f]\n",
    "images = map(loading, files)\n",
    "processed = map(preprocessing,images)\n",
    "results = map(info_extraction, processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this looks like a pretty concise way to process a pipeline! But is it the best way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to consider is that at the end of the program, `files`, `images`, `processed`, and `results` are all full arrays available to you in program memmory. This could be useful for debugging (did everything look good after preprocessing?). It may also be useful because you need to aggregated across intermediate results. Maybe you only want to use images where the background level is $\\pm 10\\%$ of the median across all images in your experiment (if Otsu's method sets the background threshold very high or low, it's suspicious).\n",
    "\n",
    "The downside of having all the intermediate information is that it may use a lot of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Not all lines above are as memory intensive as the others. Which line is the least offensive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** in our lesson on loading an image we discussed how much memory an image can use up. If you are processing 1000 16-bit, 1024x1024 pixel images each with 3 channels, how much memory does the first line above use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_bytes = 1000 * 2 * 1024**2 * 3\n",
    "print(\"{} Bytes\".format(num_bytes))\n",
    "print(\"{} GB\".format(num_bytes / 1024.**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's probably not going to go smoothly if you have a multi-stage pipeline. You could delete each stage as you go, keeping in memory only the last and current stage using `del` stagements, e.g. `del images`, but this can introduce bugs and still requires you to keep copies of everything. For very large numbers of images, this strategy suffers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to design a pipeline is to draw out the pipes. Map out the dependencies between pieces of information so you can see where to break things into functions and figure out how much data you actually need to keep in memory. The example below is a workflow for processing a red structural channel and a green calcium fluorescence channel in a 60-frame time series of images. The goal is to find cells, make masks, and track the fluorescence intensities over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram of a pipeline](../fig/pipeline_diagrams.png \"A typical pipeline diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some observations we can make once we've drawn out this plan? We can see that loading raw images and preprocessing them can be done with one function, applied image-by-image, without saving the intermediate. However we need to stop after this and max project. We can then remove all of the green channel data from memory and work on the red channel, keeping only the cell location masks. We then apply the cell location masks to each image, one at a time, take out resulting total cell intensities and save them in an array.\n",
    "\n",
    "Without doing this we may have tried to load both channels at once. This approach reduces our memory consumption by 50%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
