{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing strategies: vertical and horizontal integration\n",
    "\n",
    "Congratulations, you've just completed an awesome image processing pipeline that takes an image and does something rather useful. You've tested it a few times and you're happy with the results. Now you need to apply the same operation to 1000 images. Maybe you need to apply them to 20 datasets with slightly different parameters. Maybe you need to aggreagate information across the results of each processed image to make your conclusions. Doing this can be hard, but sometimes it's so easy you can see many ways to approach this, and are not sure which one is the best. Let's talk about what you need to keep in mind when building systems to actually apply your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** You're at the point where you have the functions below. How might you apply the pipeline you developed to every image in a folder, saving the results in a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "\n",
    "def loading(image_file_name):\n",
    "    return imread(image_file_name)\n",
    "\n",
    "def preprocessing(image):\n",
    "    return image # real code would go here\n",
    "\n",
    "def info_extraction(image):\n",
    "    results = None\n",
    "    return results\n",
    "\n",
    "data_folder = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import listdir\n",
    "\n",
    "files = listdir(data_folder)\n",
    "results = []\n",
    "\n",
    "for f in files:\n",
    "    img = loading(f)\n",
    "    processed = preprocessing(img)\n",
    "    result = info_extraction(processed)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What made this a pretty straightforward proceedure is that we encapsulated _everything_ in a function. If we developed our pipeline without doing this, we could wrap everthing in a giant function which took as parameters everything we needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whole_pipeline(image_file_name):\n",
    "    pass # code would go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not be the best idea. Maybe next week you want to load similar images the same way, preprocess them differently, then extract the same information. You could have imported the `loading` and `info_extraction` you already have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also be thinking of an alternative way of solving this problem that looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = listdir(data_folder)\n",
    "images = []\n",
    "processed = []\n",
    "results = []\n",
    "\n",
    "for f in files:\n",
    "    images.append(loading(f))\n",
    "    \n",
    "for img in images:\n",
    "    processed.append(preprocessing(img))\n",
    "    \n",
    "for proc in processed:\n",
    "    results.append(info_extraction(proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of applying all processing steps to each image one at a time, you can apply each processing step to each image. So now rather than completing your pipline \"vertically\", from start to finish, top to bottom, you are completing your pipeline \"horizontally\", applying each stage across the board in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, when you've made your code modular with little functions, Python tries to support this approach with _functional programming tools_. We'll talk about one of the especially useful ones:\n",
    "\n",
    "- `map(function, iterable)`. Applies `function` to each element in `iterable` (i.e. anything you can loop through) and gives you a new list with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = listdir(data_folder)\n",
    "images = map(loading, files)\n",
    "processed = map(preprocessing(images))\n",
    "results = map(info_extraction, processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this looks like a pretty concise way to process a pipeline! But is it the best way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to consider is that at the end of the program, `files`, `images`, `processed`, and `results` are all full arrays available to you in program memmory. This could be useful for debugging (did everything look good after preprocessing?), but may use a lot of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Not all lines above are as memory intensive as the others. Which line is the least offensive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** in our lesson on loading an image we discussed how much memory an image can use up. If you are processing 1000 16-bit, 1024x1024 pixel images each with 3 channels, how much memory does the first line above use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_bytes = 1000 * 2 * 1024**2 * 3\n",
    "print(\"{} Bytes\".format(num_bytes))\n",
    "print(\"{} GB\".format(num_bytes / 1024.**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's probably not going to go smoothly if you have a multi-stage pipeline. You could delete each stage as you go, keeping in memory only the last and current stage using `del` stagements, e.g. `del images`, but this can introduce bugs and still requires you to keep copies of everything. For very large numbers of images, this strategy suffers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory is just one resource that you have to be conscious of while designing pipelines. Speed (or time) is a concern too. You may think you are willing to wait on code for a day or two, but almost without fail your code will crash midway through and you will need to continuously iterate on your pipeline. You don't want to wait hours to verify that your list update worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've been listening to Intel's marketing, you may be thinking \"My computer has 8 cores. Can't I process 8 images at a time?\"\n",
    "\n",
    "Yes. Yes you can. Python has many tools to run code in parallel, but these tools all work by, secretly, copying your code and running another Python process on your computer. That's OK, it will manage creating, communicating with, and destroying that process, but things you may expect to work, like accessing variables you declared earlier in your code, may go poorly. However, if you've built a pipeline like we have, carefully encapsulating everything (even imports) in functions, you will be fine. Functional programming and parallel programming go hand-in-hand.\n",
    "\n",
    "To give an example of how to write programs that run in parallel, we will give a quick example using `joblib`. Other libraries exist with different tradeoffs and limitations. `ipyparallel`, `pyspark`, and `multiprocessing` are all libraries to keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_inputs(x):\n",
    "    # because this will run in parallel, we can only assume that inputs to the function will be available\n",
    "    # this includes things we imported elsewhere - we lose them when we go parallel\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tools you already know, we might apply this function to some inputs like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_values = np.random.randn(50)\n",
    "y_values = map(process_inputs, x_values)\n",
    "plt.plot(x_values, y_values, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this with joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
