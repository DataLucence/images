{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3\n",
    "\n",
    "Will be done ASAP: \n",
    "Get noisier actin signal dataset, stronger nuclear signal dataset \n",
    "Re-order processing steps\n",
    "Consider a quick discussion of different types of noise\n",
    "Consider re-organizing Day3/4 and 5/6 lesson plans into \"fixed images\" and \"dynamic images\" since it's hard to motivate pre-processing w/o what we wanted to put into \"quantification\"\n",
    "\n",
    "Associated learning goal: Preprocess image data without introducing bias in downstream analysis.\n",
    "\n",
    "- Fix shot noise in the image\n",
    "    - using a median filter\n",
    "- Background subtraction\n",
    "    - manual (as a strawman)\n",
    "    - Otsu\n",
    "    - rolling ball (show a failure)\n",
    "- Discuss the concept of a pre-processing pipeline\n",
    "- Order the pre-processing steps for the pipeline based on the goals of the project\n",
    "- Masks and morphological operations\n",
    "    - Show slicing an image with a mask\n",
    "    - Erode/dilate\n",
    "    - Open/close\n",
    "    \n",
    "Other themes to keep in mind that can be used as a foundation for this lesson: \n",
    "    -Think about the downstream analysis you want to do do on the data when deciding which preprocessing steps to do.\n",
    "    -There will be many instances when you need to determine the parameters by inspecting intermediate steps\n",
    "    -It is especially important in these instances to use consistent parameters and loop through the data and report\n",
    "    the parameters used (or better yet, make code available) so that analysis can be reproduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some boilerplate code to make it easier to access useful libraries, and to make it easier to visualize data in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style('dark', rc={'image.cmap':'inferno'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import an image file and associated metadata as we learnt yesterday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "data_drug = imread(\"../data/drug.tif\")\n",
    "data_nodrug = imread(\"../data/no_drug.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/drug.json', mode='r') as f_drug:\n",
    "    meta_drug = json.load(f_drug)\n",
    "with open('../data/no_drug.json', mode='r') as f_nodrug:\n",
    "    meta_nodrug = json.load(f_nodrug)\n",
    "\n",
    "drug_stack = {}\n",
    "nodrug_stack = {}\n",
    "for idx, channel in enumerate(meta_drug['channels']):\n",
    "    drug_stack[channel] = data_drug[:,:,idx]\n",
    "    nodrug_stack[channel] = data_nodrug[:,:,idx]    \n",
    "    print(channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's display the images to make sure everything worked as expected..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "ax[0].imshow(nodrug_stack[\"actin\"])\n",
    "ax[1].imshow(nodrug_stack['nucleus'])\n",
    "ax[2].imshow(nodrug_stack[\"your_fav_protein\"])\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "ax[0].imshow(drug_stack[\"actin\"])\n",
    "ax[1].imshow(drug_stack['nucleus'])\n",
    "ax[2].imshow(drug_stack[\"your_fav_protein\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have images of fixed cells in three channels -- cell bodies labeled with actin, nuclei labeled with DAPI?, and a third protein \"your_fav_protein\" that changes its localization upon drug treatment. Just by looking at the images it seems like the protein is shifting to the cell body from the nuclei once the drug is applied (always visualize your intermediates!), but we *Really* want to be able to see this quantitatively rather than hand-waving.\n",
    "\n",
    "Our workflow then will look something like this --\n",
    "\n",
    "1. Use the actin channel to identify the cell bodies\n",
    "2. Use the nuclear channel to determine the location of the nucleus\n",
    "3. Determine if the localization of your_fav_protein changes with drug treatment. \n",
    "\n",
    "To accomplish this, we will cover topics including background subtraction, masking, filtering, and try and gain some understanding of when it is useful and **appropriate** to use these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing pipelines: an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image data can be corrupted by a number of detector, optical, and statistical issues that obscure fluorescence quantification. Preprocessing can help correct for these problems.\n",
    "\n",
    "We want to view the fluorescence intensity of cells in this image. We will need to reliably find cell bodies, but be careful not to adjust the intensities of the pixels we're measuring for your_fav_protein.\n",
    "\n",
    "**Preprocessing misteps are a good way to get a paper retracted. We argue that it's easier to make these misteps when doing things manually, but it's not *impossible* to do it computationally. In fact if you don't check intermediate steps of your data in either case, it's no good. Always visualize your intermediates!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing background from images: manual correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background Subtraction can be a powerful way to remove unwanted signal from image data. For example, if the laser intensity was set too high (and as long as your image is not saturated, because then it's back to the microscope with you!), you can get rid of the excess signals by subtracting a number (called a \"threshold\") from every pixel in the image. There are many ways to find this number, but let's quickly check the data type of our image and make sure we're working on 16-bit unsigned integers as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drug_stack['actin'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, great! Now let's take a small slice of our image, so it's easier to visualize, and start subtracting a constant threshold from that slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "\n",
    "original_slice = drug_stack[\"actin\"][500:600,50:150]\n",
    "top = original_slice.max()\n",
    "\n",
    "@interactive\n",
    "def manual_background_subtract(bg=(0, top * 0.9, 1000)):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "    bgs = original_slice.astype('int32') - bg \n",
    "    ax[0].imshow(original_slice, vmin=0, vmax=top * 0.8)\n",
    "    ax[1].imshow(bgs, vmin=0, vmax=top * 0.8)\n",
    "manual_background_subtract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with this slider and see what happens as you subtract larger and larger thresholds from the image. Eventually, the image should basically disappear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think \"eyeballing it\" is a good way to figure out what the threshold should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated detection of background threshold using Otsu's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nobuyuki Otsu proposed a method (now very widely used) to detect the threshold. Simply put, the idea is to assume that background pixels (unwanted), and foreground pixels (your signal) will follow a bimodal distribution, i.e. that all the background pixels will be a well defined group on a histogram, which will be different from another well defined group that will be brighter, and is the signal that you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "thresh = filters.threshold_otsu(original_slice)\n",
    "print(\"Threshold is:\", thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mr. Otsu thinks this is the magic number for this *slice* of the image. Let's just take a quick look at the histogram to see if the background pixels and foreground pixels are as clearly delineated as Mr. Otsu expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(drug_stack[\"actin\"].flatten(), hist_kws={'log': True}, kde=False)\n",
    "plt.axvline(thresh, ls='--', lw=2, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite! But c'est la vie..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what can we do with this threshold? We wanted to try and identify cells in the actin channel, so let's see if we can't use this new information to our advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Masks for image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're trying to identify objects, you typically want to make a mask. A Mask is exactly what it sounds like. It's something that allows you to work on only some pixels and not others. Here, we'll essentially take most of the information in our 16-bit image, and throw it away (!). We will convert the image to **binary**, essentially reducing the bit depth to 1, and in the process, replace all the pixels below the threshold to zero, and all the pixels above the threshold to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works in practice. We can ask the computer to return every pixels that's above the threshold, and take a look at a small portion of what it spits out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a mask using thresh as the threshold\n",
    "above_thresh = original_slice > thresh\n",
    "above_thresh[45:50,45:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that above_thresh is a *boolean array*. This is a series of \"True\" or \"False\" statements, one for every pixel in the image, where True implies the pixel is above the threshold, and False implies it's below the threshold. You'll notice that we can't actually do any math with True or False statements (unless you're a philosopher), so we'll replace these with ones and zeros. We can write a \"function\" to do this easily every time with one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold_image(img, thresh):\n",
    "    output = img.copy()\n",
    "    output[img < thresh] = 0\n",
    "    output[img >= thresh] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function will take two \"arguments\", the image, and the threshold, and output a mask the same size as the image, but consisting of ones and zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masked_slice = threshold_image(original_slice,thresh)\n",
    "masked_slice.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now we've converted our image slice into an integer array of ones and zeros. Let's take a quick second to see what this does to the entire image. Notice how we need to calculate the otsu threshold again for the whole image, because the *thresh* variable contains the threshold calculated with just the *original_slice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whole_image = drug_stack[\"actin\"]\n",
    "masked_whole_image = threshold_image(whole_image, filters.threshold_otsu(whole_image))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(masked_whole_image)\n",
    "ax[1].imshow(masked_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the cell bodies have many \"holes\" in them? Even isolated pixels just hanging out where they clearly don't belong. The impudence! What can we do to get rid of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Shot Noise from your Image -- Median Fitering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better at object detection, we can leverage various properties of the pixels. With time, you will be able to leverage pretty much any property you can articulate, but for now, let's use the idea that the pixels that are hanging out in the wrong places are surrounded by other pixels that are properly classified. Let's make them listen to their neighbours. There are many ways to do this. One useful method to know is called Median filtering. It goes pixel by pixel, and replaces each pixel with the median of its surroundings. Let's load our image slice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import median_filter\n",
    "plt.imshow(original_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tech tip: These images were taken with a confocal microscope, which uses a PMT (photomultiplier tube) with high sensitivity. However, because this detector operates in a low-photon regime, shot noise (Poisson distributed) can add substantial deviation of pixel values from the local fluorescence intensities they represent. Shot noise is commonly removed with the median filter, although other rank filters exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "\n",
    "@interactive\n",
    "def apply_filter(size=(1, 21)):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    \n",
    "    # Here we implement the median filtering\n",
    "    filtered = median_filter(original_slice, size=size)\n",
    "                             \n",
    "    ax[0].imshow(original_slice)\n",
    "    ax[1].imshow(filtered)\n",
    "    dif_img = filtered.astype('int') - original_slice.astype('int')\n",
    "    \n",
    "    extreme = 10000\n",
    "    im = ax[2].imshow(dif_img, vmin=-extreme, vmax=extreme, cmap='coolwarm')\n",
    "    \n",
    "    print(\"total difference in image =\" + str(np.mean(dif_img)) + \" arbitrary units\")\n",
    "    print(\"percent change =\" + str(np.mean(dif_img)/100) + \"%\") \n",
    "apply_filter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that the size of the filter determines the value of the median value of the pixels in the output. That means, the larger the filter size, the more neighbours the filter will look at, before deciding what the new pixel value should be. A good rule of thumb when determining an appropriate filter size is that it should be the smallest filter that sufficiently flattens the visible noise in the background. Many of these operations do not have well-accepted statistical tests for determing the appropriate parameters, so care needs to be taken to record and reproduce processing steps with the same parameters. \n",
    "\n",
    "Let's choose a filter size of 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_slice = median_filter(original_slice, size=3)\n",
    "filtered_image = median_filter(whole_image, size=3)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(filtered_image)\n",
    "ax[1].imshow(filtered_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the filtering affects our mask, and compare to the mask we made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masked_filtered_slice = threshold_image(filtered_slice, filters.threshold_otsu(filtered_slice))\n",
    "masked_filtered_image = threshold_image(filtered_image, filters.threshold_otsu(filtered_image))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax[0].imshow(masked_filtered_image)\n",
    "ax[1].imshow(masked_slice)\n",
    "ax[2].imshow(masked_filtered_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, but still not perfect! What do you think would happen if we tried to apply the mask before the filter? (this could be an exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_masked_slice = median_filter(masked_slice, size=3)\n",
    "filtered_masked_image = median_filter(masked_whole_image, size=3)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax[0].imshow(masked_filtered_slice)\n",
    "ax[1].imshow(filtered_masked_slice)\n",
    "ax[2].imshow(masked_filtered_slice - filtered_masked_slice, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does doing this not make sense? (this leads to discussion of morphological operations...)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
