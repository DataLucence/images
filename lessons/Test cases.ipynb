{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do you write test cases for image processing?\n",
    "\n",
    "And by the way, what's a test case?\n",
    "\n",
    "We can guess that it might be important to test things. Testing software presents some interesting challenges, although it may seem trivial at first glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure multiplication works\n",
    "2 * 2 == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duh. But how might you test _this_ example below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure numpy's random number generator works\n",
    "import numpy as np\n",
    "rand_nums = np.random.randn(10)\n",
    "print rand_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do they look random?\n",
    "import seaborn as sns\n",
    "sns.distplot(rand_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probably won't do. What if every number alternated even-odd? We would need _a lot_ of ways of looking at the data to rule out any weird behavior. It's easy to encounter a similar situation when writing image processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing denoising procedure.\n",
    "from skimage.filters.rank import median\n",
    "from skimage.morphology import disk\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = imread(\"../data/gvwilson-brain-2014/HA_1001_t1axial1.png\")\n",
    "filtered = median(image, disk(5))\n",
    "plt.imshow(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the filter work? Are you sure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes testing image processing pipelines hard is that we are usually interested in whether or not a function solves a problem rather than produces a particular numerical result.\n",
    "\n",
    "- Does my filter make my image look less noisy\n",
    "- Did my background subtraction preserve the foreground\n",
    "- Did my clustering find things that look like cells?\n",
    "\n",
    "Some of these tests we are tempted to preform using visual inspection, e.g. \"look less noisy\", but this isn't something you can test. Typically developing a pipeline is an interative process of checking results on a small subset of images, then changing your experiment or code, checking, etc. Eventually one experimental method and one set of software parameters work for what you're trying to do.\n",
    "\n",
    "So what does it mean to write a test? Why bother if, by definition, once you're done writing code you know the code works pretty well for your experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we write tests? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you tune an image processing pipeline to find cells in an image and it works really nicely on your labmate's set of 2000 images. You want to use it on your images which came from the same experimental setup. You even verify that it runs on the first 10 images of your set of 10000. However for your paper's figures, you want to produce a list of sorted cells sizes, normalized by the biggest cell size, so you throw in a bit of code to do that called `sort_and_scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_and_scale(regions):\n",
    "    def get_size_of_region(reg):\n",
    "        return reg.area\n",
    "\n",
    "    regions_by_size = sorted(regions, key=get_size_of_region)\n",
    "    biggest_region = regions_by_size[-1] # last item in a sorted list is the biggest!\n",
    "    normalized_sizes = [region.area / biggest_region.area for region in regions]\n",
    "    return normalized_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Will this code run? Will it do what you expect it to? Will it always? What line(s) are problematic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what happens when `regions = []`, i.e. no regions were identified (maybe you used too much erosion)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you would run your code and head to the pub, discovering next morning that the code you had running overnight didn't get past the 16th image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We don't write tests against variations in the input data; we write tests to protect us against unexpected consequences of variation in the code itself while the input is fixed!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the form of writing code that checks the performance of your pipeline before it's run. We're not doing this because we think we made mistakes... _yet_. If we made mistakes in the code, we're likely to make mistakes in the tests we write just after! The reason we write tests is because next week we may decided we want to change the code, and we may introduce regressions i.e. losses in correctness, capabilities, or performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Write some code to run after any changes in the code that would allow us to discover the problem above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if sort_and_scale([]) is not []:\n",
    "    print(\"sort_and_scale did not handle an empty region list properly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language features and formalisms for testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `if` statement was designed to control flow through a functioning program, not as a method of testing. Consider what would happen if we ran the above. Python would get to `if sort_and_scale([])...` and would actually run `sort_and_scale([])`, producing an error before the `if` statement even finished!\n",
    "\n",
    "When dealing in errors, you must use `try` and `except`. `try` just says \"if the following code produces an error, jump ahead to whatever I wrote in '`except`' rather than immediately halt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try {\n",
    "    sort_and_scale([])\n",
    "} except Error {\n",
    "    print(\"sort_and_scale did not handle an empty region list properly\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tests where we want a specific output rather than lack of failure, we can use `assert`, which acts like an `if`, but automatically raises an error if a failure occurs. You can then handle this error with a `try` block if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert sort_and_scale([]) == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise** Use assertion to verify that dividing two numpy integers returns a value of integer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([1], dtype=int)\n",
    "b = np.array([2], dtype=int)\n",
    "assert (a / b).dtype == int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for testing images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall that testing is for verifying that code works as expected when _input stays fixed_. So one way to test an image processing pipeline is to take an image you used to develop the pipeline, apply some step of the pipeline, save the result. Then the actual test runs the current code and verifies that it's output looks like the saved output. Of course, if you intend the results to change, then you must account for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_stage(image):\n",
    "    # Denoise\n",
    "    from skimage.filters.rank import median\n",
    "    from skimage.morphology import disk\n",
    "    \n",
    "    denoised = median(image, disk(5))\n",
    "    \n",
    "    \n",
    "    # Smooth\n",
    "    from skimage.filters import gaussian\n",
    "    \n",
    "    smoothed = gaussian(denoised, sigma=2)\n",
    "    \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the test. \n",
    "# Do this when you finish the first working draft of your function\n",
    "from skimage.io import imread, imsave\n",
    "test_input = imread(\"../data/gvwilson-brain-2014/HA_1001_t1axial1.png\")\n",
    "test_output = preprocessing_stage(test_input)\n",
    "plt.imshow(test_output)\n",
    "imsave('../data/gvwilson-brain-2014/HA_1001_t1axial1_preprocessed.png', test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the test.\n",
    "# This might be a small script called \"test_pipeline.py\" that you run before pushing your changes to github.\n",
    "# You could easily add more tests by writing more functions and listing them in `index_of_tests`\n",
    "\n",
    "def preprocessing_test():\n",
    "    from skimage.io import imread, imsave\n",
    "    test_input = imread(\"../data/gvwilson-brain-2014/HA_1001_t1axial1.png\")\n",
    "    test_output = imread(\"../data/HA_1001_t1axial1_preprocessed.png\")\n",
    "    return preprocessing_stage(test_input) == test_output\n",
    "    \n",
    "def run_all(tests):\n",
    "    for test in tests:\n",
    "        assert test()\n",
    "        \n",
    "index_of_tests = [preprocessing]\n",
    "\n",
    "run_all(index_of_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy provides one function in particular that can be useful for working with floating point numbers. Due to small differences in people's machines, operating systems, or settings, the two images compared above may not be exactly equal. The details are complicated, but it's easy to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert 0.1 + 0.2 == 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that so? Then what, may we inquire, is $0.1 + 0.2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.1 + 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite, but this occurs due to the finite precision with which computers can represent rational numbers. To help us ignore these tiny, usually irrelevant imprecisions, Numpy supplies us with `allclose`, which is used _extensively_ in Numpy's internal development test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.allclose(0.1 + 0.2, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No trouble there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a whole world of software testing out there. Python comes with a packaged called `unittest` which helps you write tests in the formal Unit Testing style. Services like Travis will download and run your code whenever you push to Github, and can alert you when a particular branch fails a test. When you get to the stage where people you've never met are using your code and helping you write more, these tools are best practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
